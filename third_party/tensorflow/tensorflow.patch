diff --git a/tensorflow/tools/toolchains/python/python_repo.bzl b/tensorflow/tools/toolchains/python/python_repo.bzl
index 47fe64d7b7b..a01a1f19c8b 100644
--- a/tensorflow/tools/toolchains/python/python_repo.bzl
+++ b/tensorflow/tools/toolchains/python/python_repo.bzl
@@ -21,6 +21,7 @@ TF_PYTHON_VERSION = "{}"
 HERMETIC_PYTHON_VERSION = "{}"
 WHEEL_NAME = "{}"
 WHEEL_COLLAB = "{}"
+USE_PYWRAP_RULES = "False"
 """
 
 def _python_repository_impl(repository_ctx):
 diff --git a/third_party/xla/xla/backends/cpu/runtime/convolution_thunk_internal.h b/third_party/xla/xla/backends/cpu/runtime/convolution_thunk_internal.h
index 84fed6bb786..d91665ed148 100644
--- a/third_party/xla/xla/backends/cpu/runtime/convolution_thunk_internal.h
+++ b/third_party/xla/xla/backends/cpu/runtime/convolution_thunk_internal.h
@@ -338,7 +338,7 @@ void EigenGenericConv2D(
     auto num_tasks = Eigen::numext::div_ceil(feature_group_count, task_size);

     if (use_thunk_runtime) {
-      ScheduleAll(&device, num_tasks, [=, &device](Eigen::Index task_index) {
+      ScheduleAll(&device, num_tasks, [=, &device](Eigen::Index task_index) mutable {
         Eigen::Index start = task_index * task_size;
         Eigen::Index end = std::min(start + task_size, feature_group_count);
         for (Eigen::Index i = start; i < end; ++i) {
diff --git a/third_party/xla/xla/shape.cc b/third_party/xla/xla/shape.cc
index 274d32a78d8..98ab8597608 100644
--- a/third_party/xla/xla/shape.cc
+++ b/third_party/xla/xla/shape.cc
@@ -40,9 +40,7 @@ namespace xla {
 Shape::Shape() = default;
 Shape::~Shape() = default;
 Shape::Shape(const Shape&) = default;
-Shape::Shape(Shape&&) noexcept = default;
 Shape& Shape::operator=(const Shape&) = default;
-Shape& Shape::operator=(Shape&&) noexcept = default;
 
 Shape::Shape(const ShapeProto& shape_proto) {
   set_element_type(shape_proto.element_type());
diff --git a/third_party/xla/xla/shape.h b/third_party/xla/xla/shape.h
index 27a244c39fa..6554e059755 100644
--- a/third_party/xla/xla/shape.h
+++ b/third_party/xla/xla/shape.h
@@ -44,9 +44,9 @@ class Shape {
   Shape();
   ~Shape();
   Shape(const Shape&);
-  Shape(Shape&&) noexcept;
+  Shape(Shape&&) noexcept = default;
   Shape& operator=(const Shape&);
-  Shape& operator=(Shape&&) noexcept;
+  Shape& operator=(Shape&&) noexcept = default;
 
   // Construct a shape from a ShapeProto.
   explicit Shape(const ShapeProto& shape_proto);
diff --git a/third_party/xla/xla/service/memory_space_assignment/allocation_value.h b/third_party/xla/xla/service/memory_space_assignment/allocation_value.h
index ad4c9a4e22a..331ca1becf8 100644
--- a/third_party/xla/xla/service/memory_space_assignment/allocation_value.h
+++ b/third_party/xla/xla/service/memory_space_assignment/allocation_value.h
@@ -125,6 +125,8 @@ class AllocationValue {
     }
   };
 
+  AllocationValue(AllocationValue&&) noexcept = default;
+  AllocationValue& operator=(AllocationValue&&) noexcept = default;
   AllocationValue(const HloValue* value, const HloPosition& position,
                   int64_t size)
       : value_(value),
